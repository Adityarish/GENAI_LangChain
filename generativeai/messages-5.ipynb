{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f6c22ff8",
   "metadata": {},
   "source": [
    "### Messages \n",
    "##### Messages are the fundamental unit of context for model in langchain. They represent the input and output of the models, carrying both the content and metadata needed to represent the state of a conversation when interacting with an LLM. Messages are objects that contain \n",
    "\n",
    "- Role - Identifies the message type(eg. user, assistant, system, etc.)\n",
    "- Content - Represents the actual content of the message(eg. text, image, etc.)\n",
    "- Additional Metadata - Carries additional metadata about the message(eg. source, author, etc.)\n",
    "\n",
    "### LangChain provides a standard message type that works across all model providers, ensuring regardless of the model being called, the message format is consistent and easy to use.\n",
    "##### "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "1c956cf7",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/adityasingh/Projects/GEN AI INT/langChain/.venv/lib/python3.14/site-packages/langchain_core/_api/deprecation.py:25: UserWarning: Core Pydantic V1 functionality isn't compatible with Python 3.14 or greater.\n",
      "  from pydantic.v1.fields import FieldInfo as FieldInfoV1\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from langchain.chat_models import init_chat_model\n",
    "\n",
    "os.environ[\"GROQ_API_KEY\"] = os.getenv(\"GROQ_API_KEY\")\n",
    "\n",
    "model = init_chat_model(\"groq:qwen/qwen3-32b\")\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "7dee01f7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AIMessage(content='<think>\\nOkay, the user asked for information about AI. I need to start by defining AI and maybe mention its different types. Let me recall the main categoriesâ€”narrow AI versus general AI. Narrow AI is what we have now, like Siri or recommendation systems. General AI is the future concept, which we haven\\'t achieved yet. I should explain the key technologies involved, such as machine learning, deep learning, neural networks. Maybe include some applications to show the impact, like healthcare, finance, autonomous vehicles. Also, mention the history briefly, when AI was first introduced. Oh, and challenges like data requirements, computational power, ethical issues. Don\\'t forget to touch on the current trends, like generative AI with models like GPT. Make sure to keep it clear and not too technical. Let me structure this into sections: definition, types, technologies, applications, history, challenges, and current trends. Check if there\\'s anything else important, like major companies or notable researchers. Maybe mention AI ethics and regulation. Alright, that should cover a comprehensive overview without going too deep into any one area. Let me put it all together in a friendly, conversational tone.\\n</think>\\n\\nArtificial Intelligence (AI) is a rapidly evolving field that aims to create systems capable of performing tasks that typically require human intelligence. Here\\'s a structured overview to give you a clear understanding:\\n\\n---\\n\\n### **1. Definition & Scope**\\nAI refers to systems or machines that mimic human cognitive functions like **learning, reasoning, problem-solving, perception, and decision-making**. It is broadly categorized into two types:\\n- **Narrow AI (Weak AI):** Designed for specific tasks (e.g., virtual assistants like Siri, recommendation algorithms, chess-playing programs). Most current AI falls into this category.\\n- **General AI (Strong AI):** A theoretical concept where systems possess human-like intelligence and adaptability across diverse domains. This remains a future goal.\\n\\n---\\n\\n### **2. Key Technologies**\\nAI relies on several foundational technologies:\\n- **Machine Learning (ML):** Algorithms learn patterns from data without explicit programming. Examples include supervised learning (labeled data) and unsupervised learning (unlabeled data).\\n- **Deep Learning:** A subset of ML using neural networks with multiple layers to process complex data (e.g., image, speech, or text analysis).\\n- **Natural Language Processing (NLP):** Enables machines to understand and generate human language (e.g., chatbots, translation tools like Google Translate).\\n- **Computer Vision:** Focuses on interpreting visual data (e.g., facial recognition, self-driving cars).\\n- **Robotics:** Combines AI with physical systems for automation (e.g., warehouse robots, surgical assistants).\\n\\n---\\n\\n### **3. Applications**\\nAI has transformed industries through practical applications:\\n- **Healthcare:** Diagnosing diseases (e.g., cancer detection via imaging), drug discovery, personalized treatment plans.\\n- **Finance:** Fraud detection, algorithmic trading, credit scoring.\\n- **Transportation:** Autonomous vehicles (e.g., Teslaâ€™s self-driving features), traffic optimization.\\n- **Retail:** Personalized recommendations (e.g., Amazon, Netflix), inventory management.\\n- **Education:** Adaptive learning platforms (e.g., Khan Academy), automated grading.\\n- **Entertainment:** Content creation (e.g., AI-generated art, music), gaming (e.g., NPCs with realistic behavior).\\n\\n---\\n\\n### **4. Historical Context**\\n- **1950sâ€“1970s:** The term \"AI\" was coined at Dartmouth College in 1956. Early optimism led to projects like the Logic Theorist and ELIZA.\\n- **1980sâ€“1990s:** Expert systems (rule-based AI) saw limited commercial success. Machine learning began to emerge with neural networks.\\n- **2000sâ€“Present:** Breakthroughs in computational power, data availability, and algorithms (e.g., deep learning) led to AIâ€™s resurgence. Landmark achievements include IBMâ€™s Watson, AlphaGo (defeating Go champions), and large language models like GPT.\\n\\n---\\n\\n### **5. Challenges & Ethical Considerations**\\n- **Data Requirements:** High-quality, diverse data is critical but often scarce or biased.\\n- **Computational Costs:** Training advanced models (e.g., GPT-4) requires massive computational resources.\\n- **Bias & Fairness:** AI systems can inherit biases from training data, leading to ethical concerns.\\n- **Transparency & Accountability:** \"Black box\" models (e.g., deep learning) are hard to interpret, complicating trust and regulation.\\n- **Job Displacement:** Automation may disrupt labor markets, necessitating policy responses.\\n- **Security Risks:** AI can be weaponized (e.g., deepfakes, autonomous weapons), demanding robust governance.\\n\\n---\\n\\n### **6. Current Trends**\\n- **Generative AI:** Models like GPT, DALL-E, and Stable Diffusion create text, images, and videos, revolutionizing content creation.\\n- **AI for Sustainability:** Addressing climate change through energy optimization, carbon capture, and environmental monitoring.\\n- **AI in Everyday Life:** Integration into smartphones, smart homes, and wearable devices (e.g., fitness trackers).\\n- **AI Governance:** Increasing focus on regulations (e.g., EUâ€™s AI Act) to ensure ethical use and mitigate risks.\\n\\n---\\n\\n### **7. Major Players**\\n- **Tech Giants:** Google (DeepMind), Microsoft (Azure AI), Meta (Facebook AI), Amazon (AWS AI), Apple.\\n- **Startups:** Anthropic (Claude), Hugging Face, NVIDIA (AI chips), OpenAI (GPT).\\n- **Academic Institutions:** MIT, Stanford, Carnegie Mellon, and research labs like Google Brain and FAIR (Facebook AI Research).\\n\\n---\\n\\n### **8. Future Outlook**\\n- **AI-Driven Innovation:** Potential to solve global challenges (e.g., personalized medicine, clean energy).\\n- **Human-AI Collaboration:** Augmenting human capabilities rather than replacing them (e.g., AI-assisted decision-making in healthcare).\\n- **Ethical AI:** Focus on fairness, transparency, and inclusivity in development and deployment.\\n\\n---\\n\\nAI is not just a technological tool but a transformative force reshaping society. Its future will depend on balancing innovation with ethical responsibility, ensuring benefits are broadly shared and risks are mitigated. If youâ€™d like to dive deeper into any specific aspect (e.g., how AI works, ethical dilemmas, or career paths in AI), feel free to ask! ðŸ¤–', additional_kwargs={}, response_metadata={'token_usage': {'completion_tokens': 1336, 'prompt_tokens': 14, 'total_tokens': 1350, 'completion_time': 2.842608818, 'completion_tokens_details': None, 'prompt_time': 0.000383265, 'prompt_tokens_details': None, 'queue_time': 0.047027075, 'total_time': 2.842992083}, 'model_name': 'qwen/qwen3-32b', 'system_fingerprint': 'fp_5cf921caa2', 'service_tier': 'on_demand', 'finish_reason': 'stop', 'logprobs': None, 'model_provider': 'groq'}, id='lc_run--019c6c9c-6ebd-7c83-923d-d012a1a0b320-0', tool_calls=[], invalid_tool_calls=[], usage_metadata={'input_tokens': 14, 'output_tokens': 1336, 'total_tokens': 1350})"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.invoke(\"Please tell me something about AI\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72a3dadf",
   "metadata": {},
   "source": [
    "## Text Prompts\n",
    "#### Text prompts are strings - ideal for straightforward text-based tasks where you don't need to retain conversation history.\n",
    "\n",
    "#### Use Text prompts for:\n",
    "- you have a single standalone question or request\n",
    "- you don't need to conversation history\n",
    "- you want minimal code complexity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c861a609",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AIMessage(content='<think>\\nOkay, the user is asking what LangChain is. I need to start by defining it. LangChain is a framework for developing applications with language models, right? I should mention it\\'s designed to make it easy to integrate LLMs into applications.\\n\\nNext, the key components. I remember LangChain has modules like prompts, models, memory, indexes, agents, callbacks, and chains. Each of these plays a role in building and managing the application. For example, prompts help in creating structured prompts for the models, models handle the actual LLMs, memory for tracking conversation history, etc.\\n\\nThe main use cases would be building chatbots, question-answering systems, or any app requiring interaction with LLMs. Maybe give examples like customer support chatbots or personalized recommendation systems.\\n\\nI should also highlight the benefits. It simplifies integration, offers flexibility with various models, supports memory for context, and has tools for testing and deployment. Mentioning that it\\'s open-source and has a growing community would be good too.\\n\\nWait, the user might be a developer looking to start a project. They might need to know how to get started. Including a simple example would help. Like using the LangChain library to create a prompt template and run it with an LLM. But keep the code example basic so it\\'s not overwhelming.\\n\\nAlso, check if there\\'s any version or latest updates. As of my knowledge cutoff in October 2023, I should note that LangChain is actively developed, so there might be new features since then.\\n\\nMake sure the explanation is clear and not too technical. Avoid jargon where possible, but since it\\'s about a framework, some technical terms are necessary. Organize the information into sections like definition, key components, use cases, benefits, and example. Keep each section concise.\\n\\nWait, the user might confuse LangChain with other frameworks like Hugging Face\\'s Transformers or LlamaIndex. Should I mention that? Maybe briefly state that LangChain is specifically focused on chaining and orchestration of LLMs, unlike others that might focus more on model training or data indexing.\\n\\nAlso, note that LangChain supports multiple LLM providers, like OpenAI, Hugging Face, Anthropic, etc. That\\'s a key point for flexibility.\\n\\nIn the example, maybe use a simple prompt template and a fake LLM or a local one for demonstration. The user can install LangChain via pip if they want to try it out.\\n\\nCheck for any common mistakes users might make, like not setting up the correct API keys or misunderstanding how to chain multiple steps. But maybe that\\'s beyond the scope of the initial explanation.\\n\\nFinally, summarize the key points to reinforce the main ideas. Make sure the answer is structured logically and flows well from one section to the next.\\n</think>\\n\\n**LangChain** is a **framework designed to simplify the development of applications powered by large language models (LLMs)**. It provides tools, abstractions, and patterns to integrate LLMs into applications, enabling developers to build complex workflows with ease. Whether you\\'re creating chatbots, data analysis tools, or interactive AI assistants, LangChain helps streamline the integration of LLMs while offering flexibility and extensibility.\\n\\n---\\n\\n### **Key Components of LangChain**\\nLangChain is modular, with core components that work together to build robust applications:\\n\\n1. **Prompts**  \\n   - Manages how inputs are structured for LLMs (e.g., templates for dynamic input).\\n   - Example: Creating a prompt template like `\"Answer the question: {question}\"`.\\n\\n2. **Models**  \\n   - Integrates with various LLMs (e.g., OpenAI, Hugging Face, Anthropic, Google).\\n   - Supports both text-to-text and chat-based models.\\n\\n3. **Memory**  \\n   - Handles context retention across interactions (e.g., conversation history).\\n   - Useful for maintaining state in multi-turn conversations.\\n\\n4. **Indexes**  \\n   - Works with vector databases (e.g., FAISS, Chroma) for retrieval-augmented generation (RAG).\\n   - Enables LLMs to access and use external knowledge.\\n\\n5. **Agents**  \\n   - Allows LLMs to autonomously decide which tools to use (e.g., web search, database queries).\\n   - Example: A chatbot that fetches weather data or books flights.\\n\\n6. **Callbacks**  \\n   - Tracks execution and logs results for debugging or analytics.\\n\\n7. **Chains**  \\n   - Combines multiple components into a sequence (e.g., prompt â†’ model â†’ post-processing).\\n\\n8. **Tools & Utilities**  \\n   - Includes APIs for external services (e.g., Google Search, SQL databases) and utilities for testing.\\n\\n---\\n\\n### **Use Cases**\\n- **Chatbots**: Customer support, personal assistants.\\n- **Data Analysis**: Generating insights from structured or unstructured data.\\n- **RAG Systems**: Enhancing LLM responses with external data sources.\\n- **Automation**: Automating workflows using LLMs as decision-makers.\\n\\n---\\n\\n### **Benefits**\\n- **Flexibility**: Works with any LLM provider (OpenAI, Hugging Face, etc.).\\n- **Orchestration**: Simplifies complex workflows with chains and agents.\\n- **Memory Support**: Maintains context for multi-turn interactions.\\n- **Extensibility**: Modular design allows customization for specific needs.\\n- **Community & Ecosystem**: Open-source with growing tools and integrations.\\n\\n---\\n\\n### **Example: Simple LangChain Workflow**\\n```python\\nfrom langchain.chat_models import ChatOpenAI\\nfrom langchain.prompts import PromptTemplate\\nfrom langchain.chains import LLMChain\\n\\n# 1. Set up a prompt template\\nprompt_template = PromptTemplate(\\n    input_variables=[\"question\"], \\n    template=\"Answer the question: {question}\"\\n)\\n\\n# 2. Initialize an LLM\\nllm = ChatOpenAI(model=\"gpt-3.5-turbo\", temperature=0)\\n\\n# 3. Create a chain\\nchain = LLMChain(llm=llm, prompt=prompt_template)\\n\\n# 4. Run the chain\\nresult = chain.run(\"What is the capital of France?\")\\nprint(result)  # Output: \"The capital of France is Paris.\"\\n```\\n\\n---\\n\\n### **Getting Started**\\n1. Install LangChain:  \\n   ```bash\\n   pip install langchain\\n   ```\\n2. Set your API keys (e.g., `OPENAI_API_KEY`) for LLM providers.\\n3. Explore tutorials and examples in the [LangChain documentation](https://python.langchain.com/).\\n\\n---\\n\\n### **Why LangChain?**\\nLangChain stands out by unifying LLMs with other systems (e.g., databases, APIs) and providing a structured way to build applications. It reduces boilerplate code and offers tools for deployment, monitoring, and testing, making it ideal for production-grade AI systems.\\n\\nFor updates, check the [LangChain GitHub](https://github.com/langchain-ai/langchain) or community forums, as the framework evolves rapidly.', additional_kwargs={}, response_metadata={'token_usage': {'completion_tokens': 1444, 'prompt_tokens': 13, 'total_tokens': 1457, 'completion_time': 3.257707315, 'completion_tokens_details': None, 'prompt_time': 0.000364425, 'prompt_tokens_details': None, 'queue_time': 0.046372975, 'total_time': 3.25807174}, 'model_name': 'qwen/qwen3-32b', 'system_fingerprint': 'fp_5cf921caa2', 'service_tier': 'on_demand', 'finish_reason': 'stop', 'logprobs': None, 'model_provider': 'groq'}, id='lc_run--019c6c9f-633a-7b83-97f1-2d63d7454ada-0', tool_calls=[], invalid_tool_calls=[], usage_metadata={'input_tokens': 13, 'output_tokens': 1444, 'total_tokens': 1457})"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.invoke(\"What is langchain?\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b9abd684",
   "metadata": {},
   "source": [
    "## Message Prompts\n",
    "### Alternatively , you can pass in a list of messages to the model by providing a list of message objects.\n",
    "Message types\n",
    "- System message - Tell the model how to behave and provide context for interaction\n",
    "- User message - Represents user input and interaction with model\n",
    "- AI message - Responses generated by the model including text content , tool calls and metadata\n",
    "- Tool message - Represents the output of a tool call"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "055dcf0c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "content='Hi!' additional_kwargs={} response_metadata={}\n",
      "content='Hello!' additional_kwargs={} response_metadata={} tool_calls=[] invalid_tool_calls=[]\n",
      "content='You are a helpful assistant.' additional_kwargs={} response_metadata={}\n"
     ]
    }
   ],
   "source": [
    "from langchain.messages import HumanMessage, AIMessage, SystemMessage\n",
    "\n",
    "human_message = HumanMessage(content=\"Hi!\")\n",
    "ai_message = AIMessage(content=\"Hello!\")\n",
    "system_message = SystemMessage(content=\"You are a helpful assistant.\")\n",
    "\n",
    "print(human_message)\n",
    "print(ai_message)\n",
    "print(system_message)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "28301488",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<think>\n",
      "Okay, the user is asking about RAG applications. Let me start by recalling what RAG is. RAG stands for Retrieval-Augmented Generation. It's a framework that combines retrieval from a knowledge base with a language model's generation capabilities. \n",
      "\n",
      "First, I need to explain the basic components. There's the retriever, which fetches relevant documents from a database, and the generator, which uses those documents to create a response. It's important to mention that this approach helps the model stay up-to-date with the latest information and improves accuracy.\n",
      "\n",
      "I should break down the workflow into steps. Maybe start with the user's query being processed by the retriever, then the generator using the retrieved content. Examples would help. For instance, in a customer support chatbot, the system could pull up previous interactions or FAQs to form a response.\n",
      "\n",
      "Applications are key here. The user might want to know where RAG is used. Common areas include customer service, research assistance, and dynamic content generation. Need to think of specific use cases like a Q&A bot for a company's internal knowledge base or a personal assistant that accesses a user's files.\n",
      "\n",
      "Benefits should be highlighted too. RAG allows models to provide accurate, up-to-date information, which is a big plus over static models. Also, it can reduce costs by using existing knowledge bases.\n",
      "\n",
      "Wait, the user might also be interested in the challenges. Maybe mention things like the need for efficient retrieval systems, handling very large databases, or ensuring the generator doesn't produce hallucinations even when using retrieved documents.\n",
      "\n",
      "I should structure the answer with an overview, components, workflow, applications, benefits, and maybe some challenges. Make sure to use clear examples and avoid jargon where possible. Let me check if I'm missing any key points. Oh, maybe mention the integration with vector databases or similarity searches for retrieval. Also, note that RAG is different from traditional NLP in that it's dynamic and leverages external knowledge.\n",
      "</think>\n",
      "\n",
      "A **Retrieval-Augmented Generation (RAG)** application is an AI system that combines **retrieval-based methods** (fetching relevant information from a database or knowledge source) with **generative models** (like large language models, or LLMs) to produce accurate, context-aware outputs. RAG is designed to enhance the capabilities of LLMs by enabling them to leverage external data sources, improving their ability to answer questions, generate text, or perform tasks with up-to-date and domain-specific knowledge.\n",
      "\n",
      "---\n",
      "\n",
      "### **Key Components of a RAG Application**\n",
      "1. **Retriever (Knowledge Base)**:\n",
      "   - Fetches relevant documents, facts, or data from a structured or unstructured database.\n",
      "   - Uses techniques like semantic search (e.g., embeddings, vector databases) or keyword matching to find the most relevant information.\n",
      "   - Example: A vector database (e.g., FAISS, Pinecone) that stores embeddings of documents.\n",
      "\n",
      "2. **Generator (Language Model)**:\n",
      "   - A large language model (e.g., GPT, Llama, or Mistral) that synthesizes the retrieved information into a coherent answer or response.\n",
      "   - Uses the retrieved context to generate natural language outputs, ensuring alignment with the retrieved data.\n",
      "\n",
      "3. **Integration Layer**:\n",
      "   - Connects the retriever and generator, ensuring seamless flow of retrieved context into the generator.\n",
      "   - Handles tasks like filtering, ranking, or summarizing retrieved documents before passing them to the generator.\n",
      "\n",
      "---\n",
      "\n",
      "### **Workflow of a RAG Application**\n",
      "1. **User Query**:\n",
      "   - The user submits a question or task (e.g., \"What is the capital of France?\").\n",
      "\n",
      "2. **Retrieval**:\n",
      "   - The system queries a database (e.g., a knowledge base, Wikipedia, or internal documents) to retrieve relevant documents using embeddings or keyword search.\n",
      "   - For example, if the query is about climate change, the retriever might fetch recent scientific papers or reports.\n",
      "\n",
      "3. **Augmentation**:\n",
      "   - The retrieved context is combined with the original query and fed into the generator.\n",
      "\n",
      "4. **Generation**:\n",
      "   - The generator uses the retrieved information to produce a response, ensuring factual accuracy and contextual relevance.\n",
      "\n",
      "5. **Output**:\n",
      "   - The final answer is returned to the user, often with citations or references to the sources used.\n",
      "\n",
      "---\n",
      "\n",
      "### **Applications of RAG**\n",
      "1. **Customer Support Chatbots**:\n",
      "   - Retrieving FAQs, troubleshooting guides, or company policies to answer user queries in real time.\n",
      "\n",
      "2. **Research Assistance**:\n",
      "   - Helping researchers by fetching relevant papers, datasets, or findings from academic databases (e.g., arXiv, PubMed).\n",
      "\n",
      "3. **Internal Knowledge Management**:\n",
      "   - Enabling employees to access company-specific documents, project records, or technical manuals via AI-powered search.\n",
      "\n",
      "4. **Dynamic Content Generation**:\n",
      "   - Generating personalized content (e.g., product descriptions, marketing copy) by pulling from customer data or inventory systems.\n",
      "\n",
      "5. **Legal and Compliance Systems**:\n",
      "   - Providing lawyers with case law, regulations, or contractual clauses relevant to a specific query.\n",
      "\n",
      "6. **Healthcare and Medicine**:\n",
      "   - Assisting doctors by retrieving patient records, medical literature, or drug information to support diagnoses and treatments.\n",
      "\n",
      "---\n",
      "\n",
      "### **Advantages of RAG**\n",
      "- **Improved Accuracy**: Relies on real-world data to avoid hallucinations (factual errors) common in purely generative models.\n",
      "- **Up-to-Date Knowledge**: Can access the latest information from external sources (unlike static LLMs trained on outdated data).\n",
      "- **Domain-Specific Expertise**: Tailored to specific industries by retrieving domain-specific knowledge.\n",
      "- **Cost Efficiency**: Leverages existing databases, reducing the need to retrain large models for new data.\n",
      "\n",
      "---\n",
      "\n",
      "### **Challenges in RAG**\n",
      "1. **Retrieval Efficiency**:\n",
      "   - Large databases require fast and scalable search systems (e.g., vector databases like Weaviate or Milvus).\n",
      "   - High-quality embeddings are critical for semantic search.\n",
      "\n",
      "2. **Contextual Relevance**:\n",
      "   - Ensuring retrieved documents are relevant to the query and properly summarized for the generator.\n",
      "\n",
      "3. **Hallucination Mitigation**:\n",
      "   - Even with retrieval, the generator might misinterpret or combine facts incorrectly.\n",
      "\n",
      "4. **Latency**:\n",
      "   - Retrieval and generation can introduce delays, requiring optimization (e.g., caching, parallel processing).\n",
      "\n",
      "---\n",
      "\n",
      "### **Example Use Case**\n",
      "**Scenario**: A company wants a chatbot to answer questions about its products.  \n",
      "**RAG Workflow**:\n",
      "1. User asks: \"What is the warranty for your smartwatch?\"\n",
      "2. Retriever searches the company's internal product database and finds the warranty section for the smartwatch.\n",
      "3. Generator synthesizes the retrieved information into a concise answer: \"Our smartwatch comes with a 2-year limited warranty covering manufacturing defects.\"\n",
      "4. The chatbot replies to the user with the answer.\n",
      "\n",
      "---\n",
      "\n",
      "### **Tools and Frameworks for RAG**\n",
      "- **Retrieval**: Elasticsearch, FAISS, Pinecone, Weaviate.\n",
      "- **Generation**: Hugging Face Transformers, LangChain, LlamaIndex (LLamaIndex), or custom LLMs.\n",
      "- **Integration**: LangChain (a popular framework for building RAG pipelines) provides tools to connect retrievers and generators.\n",
      "\n",
      "---\n",
      "\n",
      "### **Conclusion**\n",
      "RAG applications bridge the gap between static language models and dynamic, real-world data sources. By combining retrieval and generation, they enable AI systems to deliver accurate, context-aware, and up-to-date responses, making them ideal for industries where data is constantly evolving or domain-specific expertise is critical.\n"
     ]
    }
   ],
   "source": [
    "from langchain.messages import HumanMessage, AIMessage, SystemMessage\n",
    "\n",
    "messages = [\n",
    "    SystemMessage(\"You are a data Science expert\"),\n",
    "    HumanMessage(\"Explain about RAG Application?\"),\n",
    "]\n",
    "response = model.invoke(messages)\n",
    "print(response.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "3f82fe97",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AIMessage(content='<think>\\nOkay, the user is asking about RAG applications. Let me start by recalling what RAG is. RAG stands for Retrieval-Augmented Generation. It\\'s a framework that combines retrieval from a knowledge base with a language model\\'s generation capabilities. \\n\\nFirst, I need to explain the basic components. There\\'s the retriever, which fetches relevant documents from a database, and the generator, which uses those documents to create a response. It\\'s important to mention that this approach helps the model stay up-to-date with the latest information and improves accuracy.\\n\\nI should break down the workflow into steps. Maybe start with the user\\'s query being processed by the retriever, then the generator using the retrieved content. Examples would help. For instance, in a customer support chatbot, the system could pull up previous interactions or FAQs to form a response.\\n\\nApplications are key here. The user might want to know where RAG is used. Common areas include customer service, research assistance, and dynamic content generation. Need to think of specific use cases like a Q&A bot for a company\\'s internal knowledge base or a personal assistant that accesses a user\\'s files.\\n\\nBenefits should be highlighted too. RAG allows models to provide accurate, up-to-date information, which is a big plus over static models. Also, it can reduce costs by using existing knowledge bases.\\n\\nWait, the user might also be interested in the challenges. Maybe mention things like the need for efficient retrieval systems, handling very large databases, or ensuring the generator doesn\\'t produce hallucinations even when using retrieved documents.\\n\\nI should structure the answer with an overview, components, workflow, applications, benefits, and maybe some challenges. Make sure to use clear examples and avoid jargon where possible. Let me check if I\\'m missing any key points. Oh, maybe mention the integration with vector databases or similarity searches for retrieval. Also, note that RAG is different from traditional NLP in that it\\'s dynamic and leverages external knowledge.\\n</think>\\n\\nA **Retrieval-Augmented Generation (RAG)** application is an AI system that combines **retrieval-based methods** (fetching relevant information from a database or knowledge source) with **generative models** (like large language models, or LLMs) to produce accurate, context-aware outputs. RAG is designed to enhance the capabilities of LLMs by enabling them to leverage external data sources, improving their ability to answer questions, generate text, or perform tasks with up-to-date and domain-specific knowledge.\\n\\n---\\n\\n### **Key Components of a RAG Application**\\n1. **Retriever (Knowledge Base)**:\\n   - Fetches relevant documents, facts, or data from a structured or unstructured database.\\n   - Uses techniques like semantic search (e.g., embeddings, vector databases) or keyword matching to find the most relevant information.\\n   - Example: A vector database (e.g., FAISS, Pinecone) that stores embeddings of documents.\\n\\n2. **Generator (Language Model)**:\\n   - A large language model (e.g., GPT, Llama, or Mistral) that synthesizes the retrieved information into a coherent answer or response.\\n   - Uses the retrieved context to generate natural language outputs, ensuring alignment with the retrieved data.\\n\\n3. **Integration Layer**:\\n   - Connects the retriever and generator, ensuring seamless flow of retrieved context into the generator.\\n   - Handles tasks like filtering, ranking, or summarizing retrieved documents before passing them to the generator.\\n\\n---\\n\\n### **Workflow of a RAG Application**\\n1. **User Query**:\\n   - The user submits a question or task (e.g., \"What is the capital of France?\").\\n\\n2. **Retrieval**:\\n   - The system queries a database (e.g., a knowledge base, Wikipedia, or internal documents) to retrieve relevant documents using embeddings or keyword search.\\n   - For example, if the query is about climate change, the retriever might fetch recent scientific papers or reports.\\n\\n3. **Augmentation**:\\n   - The retrieved context is combined with the original query and fed into the generator.\\n\\n4. **Generation**:\\n   - The generator uses the retrieved information to produce a response, ensuring factual accuracy and contextual relevance.\\n\\n5. **Output**:\\n   - The final answer is returned to the user, often with citations or references to the sources used.\\n\\n---\\n\\n### **Applications of RAG**\\n1. **Customer Support Chatbots**:\\n   - Retrieving FAQs, troubleshooting guides, or company policies to answer user queries in real time.\\n\\n2. **Research Assistance**:\\n   - Helping researchers by fetching relevant papers, datasets, or findings from academic databases (e.g., arXiv, PubMed).\\n\\n3. **Internal Knowledge Management**:\\n   - Enabling employees to access company-specific documents, project records, or technical manuals via AI-powered search.\\n\\n4. **Dynamic Content Generation**:\\n   - Generating personalized content (e.g., product descriptions, marketing copy) by pulling from customer data or inventory systems.\\n\\n5. **Legal and Compliance Systems**:\\n   - Providing lawyers with case law, regulations, or contractual clauses relevant to a specific query.\\n\\n6. **Healthcare and Medicine**:\\n   - Assisting doctors by retrieving patient records, medical literature, or drug information to support diagnoses and treatments.\\n\\n---\\n\\n### **Advantages of RAG**\\n- **Improved Accuracy**: Relies on real-world data to avoid hallucinations (factual errors) common in purely generative models.\\n- **Up-to-Date Knowledge**: Can access the latest information from external sources (unlike static LLMs trained on outdated data).\\n- **Domain-Specific Expertise**: Tailored to specific industries by retrieving domain-specific knowledge.\\n- **Cost Efficiency**: Leverages existing databases, reducing the need to retrain large models for new data.\\n\\n---\\n\\n### **Challenges in RAG**\\n1. **Retrieval Efficiency**:\\n   - Large databases require fast and scalable search systems (e.g., vector databases like Weaviate or Milvus).\\n   - High-quality embeddings are critical for semantic search.\\n\\n2. **Contextual Relevance**:\\n   - Ensuring retrieved documents are relevant to the query and properly summarized for the generator.\\n\\n3. **Hallucination Mitigation**:\\n   - Even with retrieval, the generator might misinterpret or combine facts incorrectly.\\n\\n4. **Latency**:\\n   - Retrieval and generation can introduce delays, requiring optimization (e.g., caching, parallel processing).\\n\\n---\\n\\n### **Example Use Case**\\n**Scenario**: A company wants a chatbot to answer questions about its products.  \\n**RAG Workflow**:\\n1. User asks: \"What is the warranty for your smartwatch?\"\\n2. Retriever searches the company\\'s internal product database and finds the warranty section for the smartwatch.\\n3. Generator synthesizes the retrieved information into a concise answer: \"Our smartwatch comes with a 2-year limited warranty covering manufacturing defects.\"\\n4. The chatbot replies to the user with the answer.\\n\\n---\\n\\n### **Tools and Frameworks for RAG**\\n- **Retrieval**: Elasticsearch, FAISS, Pinecone, Weaviate.\\n- **Generation**: Hugging Face Transformers, LangChain, LlamaIndex (LLamaIndex), or custom LLMs.\\n- **Integration**: LangChain (a popular framework for building RAG pipelines) provides tools to connect retrievers and generators.\\n\\n---\\n\\n### **Conclusion**\\nRAG applications bridge the gap between static language models and dynamic, real-world data sources. By combining retrieval and generation, they enable AI systems to deliver accurate, context-aware, and up-to-date responses, making them ideal for industries where data is constantly evolving or domain-specific expertise is critical.', additional_kwargs={}, response_metadata={'token_usage': {'completion_tokens': 1571, 'prompt_tokens': 26, 'total_tokens': 1597, 'completion_time': 4.614464132, 'completion_tokens_details': None, 'prompt_time': 0.000823717, 'prompt_tokens_details': None, 'queue_time': 0.159240108, 'total_time': 4.6152878489999996}, 'model_name': 'qwen/qwen3-32b', 'system_fingerprint': 'fp_2bfcc54d36', 'service_tier': 'on_demand', 'finish_reason': 'stop', 'logprobs': None, 'model_provider': 'groq'}, id='lc_run--019c6cad-cc93-71e3-9542-702b85d48089-0', tool_calls=[], invalid_tool_calls=[], usage_metadata={'input_tokens': 26, 'output_tokens': 1571, 'total_tokens': 1597})"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "b8732f3d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[SystemMessage(content='You are a data Science expert', additional_kwargs={}, response_metadata={}),\n",
       " HumanMessage(content='Explain about RAG Application?', additional_kwargs={}, response_metadata={})]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "messages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "16c3a872",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<think>\n",
      "Okay, I need to write a Python function to calculate the factorial of a number. Let me think about how to approach this. \n",
      "\n",
      "First, what is a factorial? The factorial of a number n is the product of all positive integers up to n. So, 5! is 5*4*3*2*1 = 120. And 0! is defined as 1. Oh right, I should handle that case too.\n",
      "\n",
      "So, the function should take an integer as input. Let me think about possible edge cases. What if the input is negative? Well, factorial isn't defined for negative numbers, so I should probably raise an error in that case.\n",
      "\n",
      "So the steps are: check if the input is a negative integer. If so, raise a ValueError. Then, handle the base case where n is 0 or 1, returning 1. For other values, compute the product iteratively.\n",
      "\n",
      "Wait, what about the different ways to implement it? I can do it iteratively or recursively. Since recursion can lead to stack overflow for large numbers, maybe an iterative approach is better here. Plus, for Python, handling loops for even large n (like 1000) is manageable.\n",
      "\n",
      "Let me outline the function:\n",
      "\n",
      "def factorial(n):\n",
      "    if n < 0:\n",
      "        raise ValueError(\"Factorial is not defined for negative numbers.\")\n",
      "    result = 1\n",
      "    for i in range(2, n+1):\n",
      "        result *= i\n",
      "    return result\n",
      "\n",
      "Wait, but for n=0 or n=1, the loop won't run because range(2, 0+1) would be from 2 to 1, which is empty. So in those cases, result remains 1, which is correct. Let me test this logic with some examples.\n",
      "\n",
      "Test case 1: n=0. Returns 1. Correct.\n",
      "\n",
      "Test case 2: n=1. Returns 1. Correct.\n",
      "\n",
      "Test case 3: n=5. The loop runs for 2,3,4,5. Let's see:\n",
      "\n",
      "Initially result is 1.\n",
      "\n",
      "Multiply by 2: 2.\n",
      "\n",
      "Multiply by 3: 6.\n",
      "\n",
      "Multiply by 4: 24.\n",
      "\n",
      "Multiply by 5: 120. Correct.\n",
      "\n",
      "What about n=2? The loop runs once (i=2). Result is 1*2=2. Correct.\n",
      "\n",
      "Okay, that seems to work.\n",
      "\n",
      "Alternatively, I could use math.prod, but maybe the user wants an implementation from scratch. Since the question says \"write a function\", probably better to implement it manually instead of using math module.\n",
      "\n",
      "But wait, what about using the math module's factorial function? Well, Python's math does have a factorial function in 3.9 and above. But if the user wants an implementation from scratch, that's another thing. Since the problem says \"write a Python function to calculate...\", I think they want me to code it, not just call math.factorial.\n",
      "\n",
      "So the code I outlined earlier is better.\n",
      "\n",
      "Another thing: what about n being a non-integer? Like a float. Well, the problem says \"a number\", but in Python, functions usually receive the types they are given. However, factorials are defined for integers. So perhaps check if the input is an integer. Wait, but the problem says \"a number\", but maybe the function is supposed to take an integer. The problem says to \"write a function\", probably assuming that the input is a non-negative integer. So perhaps in the function, we can check if the input is an integer. But maybe the function is supposed to handle only integers. However, in Python, if someone passes a float like 5.0, it's possible. So maybe we should check for integer-ness. Hmm.\n",
      "\n",
      "But the problem statement says \"calculate the factorial of a number\"â€”but in mathematics, factorials are for non-negative integers. So perhaps the function expects an integer. So perhaps in the function, we can check if the input is an integer. For example, if someone passes 5.0, perhaps we can convert it to integer. But if it's something like 5.5, then it's invalid. But maybe the function is supposed to take an integer. So perhaps the function should check if the input is an integer. Let me think.\n",
      "\n",
      "But perhaps the function is written with the assumption that the input is a non-negative integer. However, to make it robust, we can add checks.\n",
      "\n",
      "But the problem statement may not require that. For the scope of this problem, maybe we can assume that the input is a non-negative integer. So perhaps the function can check if it's an integer. Let me adjust the code.\n",
      "\n",
      "Wait, for example, if someone calls factorial(5.5), it's a problem. But how to check for that? In Python, we can check if the input is an integer. So perhaps, add a check that n is an instance of int. But what if someone passes a float that is integer-like, like 5.0?\n",
      "\n",
      "So perhaps:\n",
      "\n",
      "if not isinstance(n, int):\n",
      "    raise TypeError(\"Input must be an integer.\")\n",
      "\n",
      "But wait, what about if someone passes a string? Well, Python will throw a TypeError when trying to process it. But perhaps the function should handle that. However, perhaps the function is intended to be simple, and the user is responsible for passing correct input. But adding checks makes it more robust.\n",
      "\n",
      "But since the problem statement says \"write a function to calculate the factorial of a number\", perhaps it's acceptable to assume that the input is a non-negative integer, and raise appropriate errors otherwise.\n",
      "\n",
      "So, adding to the code:\n",
      "\n",
      "def factorial(n):\n",
      "    if not isinstance(n, int):\n",
      "        raise TypeError(\"Input must be an integer.\")\n",
      "    if n < 0:\n",
      "        raise ValueError(\"Factorial is not defined for negative numbers.\")\n",
      "    result = 1\n",
      "    for i in range(2, n+1):\n",
      "        result *= i\n",
      "    return result\n",
      "\n",
      "But maybe the problem doesn't require the type checking. However, including it makes the function more robust. However, perhaps the user wants to keep it simple. Since the question doesn't specify handling such cases, maybe it's better to proceed with the basic version. Let me check the initial code again.\n",
      "\n",
      "Original code:\n",
      "\n",
      "def factorial(n):\n",
      "    if n < 0:\n",
      "        raise ValueError(\"Factorial is not defined for negative numbers.\")\n",
      "    result = 1\n",
      "    for i in range(2, n+1):\n",
      "        result *= i\n",
      "    return result\n",
      "\n",
      "But if n is not an integer, this code would have issues. For example, if n is 5.5, then passing to the function would not check. However, in Python, range(2, 5.5 +1) would be range(2, 6.5), which in Python 3, range expects integer arguments. So if n is not an integer, the code would throw an error when creating the range. For example, if n is a float, then when the code tries to generate the range, it would raise a TypeError. For example, if n is 5.5, then n+1 is 6.5, and the range function would get a float. But range expects integer arguments, so in Python 3, this would raise a TypeError. So the code would fail at that point. Therefore, perhaps the function as written would implicitly check for integer inputs, because otherwise, the range would fail. However, if someone passes a string, it would fail in the first check (n < 0) with a TypeError. \n",
      "\n",
      "But perhaps it's better to explicitly check if the input is an integer. Let me add that.\n",
      "\n",
      "But how to check? Well, perhaps using isinstance(n, int). However, in Python, booleans are a subclass of integers. So, for example, True is 1, False is 0. So if someone passes True, which is 1, then the code would treat it as 1. But in Python, isinstance(True, int) returns True. So, perhaps we need to check if it's an instance of int and not a boolean. Wait, but booleans are a subclass of int. So, for example, if someone passes True (which is 1), then factorial(True) would compute 1! = 1. But maybe that's acceptable. However, perhaps the function should check that the input is an integer and not a boolean. But maybe that's overkill. The problem says \"a number\"â€”but perhaps the function is intended to take an integer. \n",
      "\n",
      "But perhaps the user wants to handle only integers, so adding a check for that. Let me adjust the code to include that:\n",
      "\n",
      "def factorial(n):\n",
      "    if not isinstance(n, int):\n",
      "        raise TypeError(\"Input must be an integer.\")\n",
      "    if n < 0:\n",
      "        raise ValueError(\"Factorial is not defined for negative numbers.\")\n",
      "    result = 1\n",
      "    for i in range(2, n+1):\n",
      "        result *= i\n",
      "    return result\n",
      "\n",
      "But what if someone passes a boolean? For example, factorial(True) would be treated as factorial(1). Well, since True is an instance of int in Python. So, for example, in Python:\n",
      "\n",
      ">>> isinstance(True, int)\n",
      "True\n",
      "\n",
      "So, if the user wants to pass booleans, perhaps that's an edge case. But perhaps the function is supposed to take integers. So perhaps we can check that the type is exactly int? But that's not straightforward. Alternatively, we can check if it's an integer and not a boolean. For example:\n",
      "\n",
      "if type(n) is not int:\n",
      "    raise TypeError(...)\n",
      "\n",
      "But then, if someone passes a subclass of int, it would fail. But perhaps for the purposes of this function, we can do that. Let me adjust:\n",
      "\n",
      "def factorial(n):\n",
      "    if type(n) is not int:\n",
      "       \n"
     ]
    }
   ],
   "source": [
    "#### Example 2\n",
    "\n",
    "system_msg = SystemMessage(\"You are a helpful coding assistant\")\n",
    "\n",
    "\n",
    "messages = [system_msg, HumanMessage(\"Write a python function to calculate the factorial of a number\")]\n",
    "response = model.invoke(messages)\n",
    "print(response.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "7bcfcd53",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<think>\n",
      "Okay, the user wants to create a REST API app using FastAPI. Let me start by outlining the basic steps. First, they need to install FastAPI and an ASGI server like Uvicorn. Then set up a main file with the FastAPI instance. \n",
      "\n",
      "I should explain how to define endpoints with different HTTP methods. Maybe start with a simple GET route for the root. Then show how to add more routes, like a POST for creating items. Path parameters and query parameters are important too. \n",
      "\n",
      "Including a data model with Pydantic would be good. It helps with data validation. Maybe create an Item model with name and description. \n",
      "\n",
      "Then, structure the example code step by step. Show how to run the server using uvicorn. Maybe mention the auto-generated docs at /docs and /redoc. \n",
      "\n",
      "Testing the API with curl or Postman could be useful. Also, note the benefits like async support and automatic documentation. \n",
      "\n",
      "Wait, should I include an example with a database? Maybe not for the initial explanation to keep it simple. Focus on in-memory storage first. Then perhaps mention how to integrate a database later. \n",
      "\n",
      "Also, make sure to explain the code parts clearly, like @app.get, @app.post, and how Pydantic models work. Highlight type hints and validation. \n",
      "\n",
      "Check if there's anything else the user might need. Maybe error handling? But again, keep it simple for now. The user can expand later. \n",
      "\n",
      "Okay, putting it all together in a step-by-step guide with code examples. Make sure the code is correct and tested. Mention the command to run the app and access the docs. That should cover the basics.\n",
      "</think>\n",
      "\n",
      "To create a REST API app with FastAPI, follow these steps:\n",
      "\n",
      "### 1. Install Dependencies\n",
      "```bash\n",
      "pip install fastapi uvicorn\n",
      "```\n",
      "\n",
      "### 2. Create `main.py`\n",
      "```python\n",
      "from fastapi import FastAPI\n",
      "from pydantic import BaseModel\n",
      "\n",
      "app = FastAPI()\n",
      "\n",
      "# Data model\n",
      "class Item(BaseModel):\n",
      "    name: str\n",
      "    description: str = None\n",
      "\n",
      "# In-memory database\n",
      "items_db = {}\n",
      "\n",
      "# Root endpoint\n",
      "@app.get(\"/\")\n",
      "def read_root():\n",
      "    return {\"message\": \"Welcome to FastAPI\"}\n",
      "\n",
      "# Create Item (POST)\n",
      "@app.post(\"/items/\")\n",
      "def create_item(item: Item):\n",
      "    items_db[item.name] = item\n",
      "    return {\"status\": \"success\", \"item\": item.name}\n",
      "\n",
      "# Read Item (GET)\n",
      "@app.get(\"/items/{item_name}\")\n",
      "def read_item(item_name: str):\n",
      "    if item := items_db.get(item_name):\n",
      "        return item\n",
      "    return {\"error\": f\"Item {item_name} not found\"}\n",
      "\n",
      "# Update Item (PUT)\n",
      "@app.put(\"/items/{item_name}\")\n",
      "def update_item(item_name: str, updated_item: Item):\n",
      "    if item_name in items_db:\n",
      "        items_db[item_name] = updated_item\n",
      "        return {\"status\": \"updated\", \"item\": item_name}\n",
      "    return {\"error\": f\"Item {item_name} not found\"}\n",
      "\n",
      "# Delete Item (DELETE)\n",
      "@app.delete(\"/items/{item_name}\")\n",
      "def delete_item(item_name: str):\n",
      "    if item_name in items_db:\n",
      "        del items_db[item_name]\n",
      "        return {\"status\": \"deleted\", \"item\": item_name}\n",
      "    return {\"error\": f\"Item {item_name} not found\"}\n",
      "```\n",
      "\n",
      "### 3. Run the Server\n",
      "```bash\n",
      "uvicorn main:app --reload\n",
      "```\n",
      "\n",
      "### Key Features Used:\n",
      "1. **Type Hints**: Python 3.10+ type hints for automatic request parsing\n",
      "2. **Pydantic Models**: `Item` class auto-validates JSON input/output\n",
      "3. **Automatic Docs**: Access `http://localhost:8000/docs` for interactive API docs\n",
      "4. **Path Operations**: Decorators like `@app.get()` define HTTP methods\n",
      "5. **Path Parameters**: `{item_name}` in routes for dynamic URL values\n",
      "6. **Query Parameters**: Add `?query=param` to endpoints for filtering/sorting\n",
      "\n",
      "### Testing the API\n",
      "Use `curl` or Postman to test endpoints:\n",
      "```bash\n",
      "# Create item\n",
      "curl -X POST \"http://localhost:8000/items/\" -H \"Content-Type: application/json\" -d '{\"name\":\"Chair\", \"description\":\"Wooden chair\"}'\n",
      "\n",
      "# Get item\n",
      "curl \"http://localhost:8000/items/Chair\"\n",
      "```\n",
      "\n",
      "### Why FastAPI?\n",
      "- **Performance**: ~3x faster than Flask for API workloads\n",
      "- **Type Safety**: Compile-time error detection\n",
      "- **Async Support**: Easily add async routes with `@app.get(\"/async\", response_model=ModelName)`\n",
      "- **Zero Configuration**: Auto-generated OpenAPI and JSON Schema\n"
     ]
    }
   ],
   "source": [
    "#### Detailed Info to LLM with system Message\n",
    "\n",
    "from langchain.messages import SystemMessage, HumanMessage, AIMessage\n",
    "\n",
    "system_message = SystemMessage(\"\"\"\n",
    "You are a senior Python developer with expertise in web frameworks. \n",
    "Always provide code examples and explain you reasoning. \n",
    "Be concise but through in your explantions.\n",
    "\"\"\")\n",
    "\n",
    "messages = [system_message , HumanMessage(\"How can we create Rest Api App using FastAPI?\")]\n",
    "response = model.invoke(messages)\n",
    "print(response.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d6ae555d",
   "metadata": {},
   "source": [
    "### Message Metadata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "0d748268",
   "metadata": {},
   "outputs": [],
   "source": [
    "human_msg = HumanMessage(\n",
    "    content=\"hello!\", \n",
    "    name=\"aditya\",\n",
    "    id= \"msg-1\"\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "ef9dc1aa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<think>\n",
      "Okay, the user said \"hello!\". I need to respond appropriately. Since they started with a greeting, I should acknowledge it warmly. Maybe say hello back and ask how I can assist them. Keep it friendly and open-ended. Let me make sure the tone is welcoming and not too formal. Alright, that should cover it.\n",
      "</think>\n",
      "\n",
      "Hello! How can I assist you today? ðŸ˜Š\n"
     ]
    }
   ],
   "source": [
    "response = model.invoke([human_msg])\n",
    "print(response.content)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "77836015",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<think>\n",
      "Okay, the user asked, \"What's 2 + 2?\" Let me think. They already said \"Great!\" after I offered help, so they're probably in a good mood. Maybe they're just testing me or want a quick answer. But since it's a simple question, I should make sure to answer it clearly.\n",
      "\n",
      "Wait, sometimes people ask this question to check if someone is a human versus an AI, but the user is already talking to me, so they know I'm an AI. Maybe they're just starting out and want to confirm basic functionality. I should respond confidently. The answer is obviously 4, but maybe they want a bit more. Should I explain it? Probably not necessary, but maybe add a friendly note. Like, \"The answer is 4! Is there anything else I can assist you with?\" That way, it's helpful and opens the door for more questions. Yeah, that sounds good. Let me go with that.\n",
      "</think>\n",
      "\n",
      "The answer is **4**! Is there anything else I can assist you with? ðŸ˜Š\n"
     ]
    }
   ],
   "source": [
    "from langchain.messages import HumanMessage, AIMessage, SystemMessage\n",
    "\n",
    "ai_msg  = AIMessage(\"I'd be happy to help you with that question!\")\n",
    "\n",
    "messages = [\n",
    "    SystemMessage(\"You are a helpful assistant.\"),\n",
    "    HumanMessage(\"Can you help me ?\"),\n",
    "    ai_msg,\n",
    "    HumanMessage(\"Great! What's 2 + 2?\")\n",
    "]\n",
    "\n",
    "response = model.invoke(messages)\n",
    "print(response.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "443cbf2a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.messages import HumanMessage, AIMessage\n",
    "from langchain.messages import ToolMessage\n",
    "\n",
    "# After a model makes a tool call\n",
    "#(Here , we demonstrate manually creating the message for brevity)\n",
    "\n",
    "ai_msg = AIMessage(\n",
    "    content=[],\n",
    "    tool_calls=[\n",
    "        {\n",
    "            \"name\" : \"get_weather\",\n",
    "            \"args\":{\"location\":\"Delhi\"},\n",
    "            \"id\":\"call_123\" # tool call id for ai_msg and tool message should be same\n",
    "        }\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "279cf6c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "weather_result = \"Sunny, 75Â°F\"\n",
    "tool_message = ToolMessage(\n",
    "    content=weather_result,\n",
    "    tool_call_id=\"call_123\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "d21c6f78",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<think>\n",
      "Okay, the user asked for the weather in Delhi. I called the get_weather function with location set to Delhi. The response came back as sunny with 75Â°F. Now I need to present this information clearly. Let me check if there's any additional details I should include, like humidity or wind speed, but the response only has temperature and condition. I should keep it simple and friendly. Maybe add an emoji for the sun. Make sure to mention the temperature in both Fahrenheit and Celsius if possible, but the user only provided Fahrenheit. Wait, the user might prefer Celsius, but since the function returned 75Â°F, I'll stick with that unless instructed otherwise. Alright, the response should be concise and easy to read. Let me put it all together.\n",
      "</think>\n",
      "\n",
      "The weather in Delhi is currently **sunny â˜€ï¸** with a temperature of **75Â°F**. It's a perfect day to enjoy outdoor activities!\n"
     ]
    }
   ],
   "source": [
    "messages = [HumanMessage(\"What's the weather in Delhi?\"),\n",
    "ai_msg, \n",
    "tool_message\n",
    "]\n",
    "\n",
    "response = model.invoke(messages)\n",
    "print(response.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5c954ed",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.14.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
